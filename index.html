<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta http-equiv="Cache-control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  <title>Face Landmarker 0.1.45</title>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three/examples/js/loaders/GLTFLoader.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm/vision_wasm_internal.js"></script>
  <style>
    body, html {
      margin: 0;
      padding: 0;
      overflow: hidden;
      height: 100%;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    #liveView {
      width: 100vw;
      height: 100vh;
      position: relative;
    }
    video, canvas {
      width: 100%;
      height: 100%;
      object-fit: cover;
      position: absolute;
      top: 0;
      left: 0;
    }
  </style>
</head>
<body>
  <div id="liveView" class="videoView">
    <video id="webcam" autoplay playsinline></video>
    <canvas class="output_canvas" id="output_canvas"></canvas>
  </div>

  <script type="module">
    import { FaceLandmarker, FilesetResolver } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest';

    const videoElement = document.getElementById('webcam');
    const canvasElement = document.getElementById('output_canvas');
    const canvasCtx = canvasElement.getContext('2d');

    const fov = 90;
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(fov, window.innerWidth / window.innerHeight, 0.1, 1000);

    const renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio);
    document.body.appendChild(renderer.domElement);

    const light = new THREE.AmbientLight(0xffffff, 1);
    scene.add(light);

    const loader = new THREE.GLTFLoader();
    const textureLoader = new THREE.TextureLoader();

    let hatModel, tonakaiModel;

    const hatParams = {
      scale: 0.5,
      positionOffset: { x: 0, y: 0, z: -1.5 },
      rotationOffset: { pitch: 0, yaw: 0, roll: 0 } // 回転オフセット
    };

    const tonakaiParams = {
      scale: 0.8,
      positionOffset: { x: 0, y: -0.8, z: -1.4 },
      rotationOffset: { pitch: 0.1, yaw: 0, roll: 0 } // 回転オフセット
    };

    let faceLandmarker;
    const faceModels = {};

    const loadModels = async () => {
      return Promise.all([
        loader.loadAsync('hat.glb').then((gltf) => {
          hatModel = gltf.scene;
          hatModel.scale.set(hatParams.scale, hatParams.scale, hatParams.scale);
        }),
        loader.loadAsync('tonakai.glb').then((gltf) => {
          tonakaiModel = gltf.scene;
          tonakaiModel.scale.set(tonakaiParams.scale, tonakaiParams.scale, tonakaiParams.scale);

          const texture = textureLoader.load('tonakai_tex.jpg');
          tonakaiModel.traverse((child) => {
            if (child.isMesh) {
              child.material = new THREE.MeshStandardMaterial({ map: texture });
            }
          });
        }),
      ]);
    };

    function calculateFaceRotation(landmarks) {
      const leftEye = landmarks[33];
      const rightEye = landmarks[263];
      const nose = landmarks[1];

      const dx = rightEye.x - leftEye.x;
      const dy = rightEye.y - leftEye.y;
      const dz = nose.z - (rightEye.z + leftEye.z) / 2;

      const yaw = Math.atan2(dy, dx);
      const pitch = Math.atan2(dz, dx);
      const roll = -Math.atan2(dy, dx);

      return { yaw, pitch, roll };
    }

    function applyModelTransform(model, position, rotation, rotationOffset) {
      if (!model) return;
      model.position.set(position.x, position.y, position.z);
      model.rotation.set(
        rotation.pitch + rotationOffset.pitch,
        rotation.yaw + rotationOffset.yaw,
        rotation.roll + rotationOffset.roll
      );
    }

    async function setupFaceLandmarker() {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );

      faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
          delegate: "GPU"
        },
        runningMode: "VIDEO",
        numFaces: 5
      });

      navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } })
        .then((stream) => {
          videoElement.srcObject = stream;
          videoElement.addEventListener("loadeddata", predictWebcam);
        })
        .catch(console.error);
    }

    function displayResults(results) {
      if (!results.faceLandmarks) return;

      const aspectRatio = videoElement.videoWidth / videoElement.videoHeight;

      results.faceLandmarks.forEach((landmarks, index) => {
        const forehead = landmarks[10];
        if (!forehead) return;

        if (!faceModels[index]) {
          const useHat = Math.random() < 0.5;
          faceModels[index] = {
            model: (useHat ? hatModel : tonakaiModel).clone(),
            params: useHat ? hatParams : tonakaiParams
          };
          scene.add(faceModels[index].model);
        }

        const { model, params } = faceModels[index];
        const position = {
          x: (forehead.x - 0.5) * 2 * aspectRatio + params.positionOffset.x,
          y: -(forehead.y - 0.5) * 2 + params.positionOffset.y,
          z: params.positionOffset.z + forehead.z * 0.1
        };

        const rotation = calculateFaceRotation(landmarks);
        applyModelTransform(model, position, rotation, params.rotationOffset);
      });

      renderer.render(scene, camera);
    }

    async function predictWebcam() {
      if (!faceLandmarker) return;

      const results = await faceLandmarker.detectForVideo(videoElement, performance.now());
      displayResults(results);
      requestAnimationFrame(predictWebcam);
    }

    window.addEventListener("resize", () => {
      const width = window.innerWidth;
      const height = window.innerHeight;

      camera.aspect = width / height;
      camera.updateProjectionMatrix();

      renderer.setSize(width, height);
      renderer.setPixelRatio(window.devicePixelRatio);
    });

    loadModels().then(() => {
      setupFaceLandmarker();
    });
  </script>
</body>
</html>
