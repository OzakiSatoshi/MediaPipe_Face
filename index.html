<!DOCTYPE html> 
<html lang="ja"> 
<head>
  <meta charset="utf-8">
  <meta http-equiv="Cache-control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  <title>Face Landmarker 0.1.19</title>

  <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
  <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
  <!-- Three.jsの読み込み -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
  <!-- GLTFLoaderの読み込み -->
  <script src="https://cdn.jsdelivr.net/npm/three/examples/js/loaders/GLTFLoader.js"></script>
  <!-- MediaPipe Vision用のWASMスクリプト -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm/vision_wasm_internal.js"></script>

  <style>
    body, html {
      margin: 0;
      padding: 0;
      overflow: hidden;
      height: 100%;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    #liveView {
      width: 100vw;
      height: 100vh;
      position: relative;
    }

    video, canvas {
      width: 100%;
      height: 100%;
      object-fit: cover;
      position: absolute;
      top: 0;
      left: 0;
    }
  </style>
</head>
<body>
  <div id="liveView" class="videoView">
    <video id="webcam" autoplay playsinline></video>
    <canvas class="output_canvas" id="output_canvas"></canvas>
  </div>

  <script type="module">
    import { FaceLandmarker, FilesetResolver } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest';

    const videoElement = document.getElementById('webcam');
    const canvasElement = document.getElementById('output_canvas');
    const canvasCtx = canvasElement.getContext('2d');
    let faceLandmarker;
    let runningMode = "VIDEO";
    let lastVideoTime = -1;

    // Three.jsのセットアップ
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
    const renderer = new THREE.WebGLRenderer({ alpha: true });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.domElement.style.position = "absolute";
    renderer.domElement.style.top = "0";
    renderer.domElement.style.left = "0";
    document.body.appendChild(renderer.domElement);

    // 照明の設定
    const light = new THREE.AmbientLight(0xffffff, 1);
    scene.add(light);

    let hatModel;
    const loader = new THREE.GLTFLoader();
    loader.load('hat.glb', (gltf) => {
      hatModel = gltf.scene;
      hatModel.scale.set(0.4, 0.4, 0.4); // サイズを倍に設定
      scene.add(hatModel);
    });

    // 顔の傾きを計算して帽子の回転に適用
    function calculateFaceRotation(landmarks) {
      const leftEye = landmarks[33];
      const rightEye = landmarks[263];
      const nose = landmarks[1];

      const dxEye = rightEye.x - leftEye.x;
      const dyEye = rightEye.y - leftEye.y;

      const yaw = -Math.atan2(dyEye, dxEye); // 左右の傾きを反転
      const pitch = Math.atan2(nose.z, dxEye);
      const roll = -Math.atan2(dyEye, dxEye); // Rollの反転

      return { yaw, pitch, roll };
    }

    function createFaceOcclusionMesh(landmarks) {
      const geometry = new THREE.BufferGeometry();
      const vertices = new Float32Array(landmarks.length * 3);

      for (let i = 0; i < landmarks.length; i++) {
          vertices[i * 3] = (landmarks[i].x - 0.5) * 2;
          vertices[i * 3 + 1] = -(landmarks[i].y - 0.5) * 2;
          vertices[i * 3 + 2] = landmarks[i].z * 0.1;
      }
      
      geometry.setAttribute('position', new THREE.BufferAttribute(vertices, 3));
      
      const material = new THREE.MeshBasicMaterial({
          color: 0x000000,
          opacity: 0.5,
          transparent: true,
          depthTest: true
      });
      
      const mesh = new THREE.Mesh(geometry, material);
      return mesh;
    }

    function updateOcclusionMesh(mesh, landmarks) {
      const positions = mesh.geometry.attributes.position.array;
      for (let i = 0; i < landmarks.length; i++) {
          positions[i * 3] = (landmarks[i].x - 0.5) * 2;
          positions[i * 3 + 1] = -(landmarks[i].y - 0.5) * 2;
          positions[i * 3 + 2] = landmarks[i].z * 0.1;
      }
      mesh.geometry.attributes.position.needsUpdate = true;
    }

    let occlusionMesh;

    async function setupFaceLandmarker() {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );

      faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        runningMode: runningMode,
        numFaces: 5 // 最大5つの顔を認識
      });

      const constraints = {
        video: {
          facingMode: "user"
        }
      };

      navigator.mediaDevices.getUserMedia(constraints)
        .then((stream) => {
          videoElement.srcObject = stream;
          videoElement.addEventListener("loadeddata", predictWebcam);
        })
        .catch((err) => {
          console.error("カメラの取得に失敗しました:", err);
        });
    }

    let lastVideoTimestamp = 0;
    async function predictWebcam() {
      if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await faceLandmarker.setOptions({ runningMode: "VIDEO" });
      }
      let startTimeMs = performance.now();

      if (lastVideoTime !== videoElement.currentTime) {
        lastVideoTime = videoElement.currentTime;
        const results = await faceLandmarker.detectForVideo(videoElement, startTimeMs);
        displayResults(results);
      }

      window.requestAnimationFrame(predictWebcam);
    }

    function displayResults(results) {
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

      if (results.faceLandmarks && hatModel) {
        results.faceLandmarks.forEach((landmarks) => {
          const forehead = landmarks[10]; // 額のランドマーク (例として index 10)
          if (forehead) {
            const x = (forehead.x - 0.5) * 2;
            const y = -(forehead.y - 0.5) * 2;
            const z = -1.5 + forehead.z * 0.1; // z座標を奥に調整しつつ、深度も加味

            hatModel.position.set(x, y + 0.3, z); // y座標をさらに上げて頭にしっかり配置

            // 顔の傾きを計算し、帽子モデルに適用
            const { yaw, pitch, roll } = calculateFaceRotation(landmarks);
            hatModel.rotation.set(pitch, yaw, roll);
          }

          if (!occlusionMesh) {
            occlusionMesh = createFaceOcclusionMesh(landmarks);
            scene.add(occlusionMesh);
          }
          updateOcclusionMesh(occlusionMesh, landmarks);
        });

        renderer.render(scene, camera);
      }
    }

    setupFaceLandmarker();
  </script>
</body>
</html>
